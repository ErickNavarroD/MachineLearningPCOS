---
title: "PCOS modeling"
author: "Erick Navarro"
date: "2023-01-25"
output: 
  pdf_document:
    toc: true
    toc_depth: 4
---
# Creating the classificators

This report contains the details of the generation and comparison of several classificators whose aim is to classify people with and without Polycystic ovary syndrome (PCOS). The goal of this project is to develop and validate a model that takes easily collected variables, and test its performance against models that possess variables obtained with increasing invasive proceedures. For a summary of the results and a clearer picture of the project, please read the Final_report.pdf file in this repository. 

## Setup
### Load packages and data 
```{r setup, include=FALSE}
library(tidyverse)
library(glmnet)
library(caret)
library(here)
library(mice)
library(DataExplorer)

#Make the code reproducible
set.seed(504)
#load the data
load(here("data.Rdata"))
# See how the data looks like
glimpse(data)
```

### Data splitting 

After loading the data and packages, we will proceed to split the data into training and validation sets.

```{r}
data = data %>% 
  column_to_rownames("id")

train.index <- caret::createDataPartition(data$pcos, p = .7, list=FALSE)

train <- data[ train.index,]
valid  <- data[-train.index,]


#Check the negative/positive ration in the validation set
table(train$pcos)
table(train$pcos)[1]/ table(train$pcos)[2]

#Check the negative/positive ration in the validation set
table(valid$pcos)
table(valid$pcos)[1]/ table(valid$pcos)[2]

#Check the negative/positive ration in the original data set
table(data$pcos)[1]/ table(data$pcos)[2]

```

### Imputation on training set

Following the data splitting, we will impute missing values. As we observed in the EDA analysis, the proportion of missing data is overall very low, which makes all the variables with misingness suitable for imputation

```{r}
# Explore misingness in training set 
sapply(train, function(x) sum(is.na(x)))
```

```{r, message=FALSE}
chained_train = mice::mice(train)

#mice::densityplot(chained_train)

chained_train = complete(data=chained_train)

sapply(chained_train, function(x) sum(is.na(x)))
```



### Define the models

Before doing the modeling, I will remove the variable "cycle_r_i", since I could find no information about what it is measuring neither in the documentation of the dataset nor online. Also, I will make the patient_file_no the rownames of the data set. 

### Logistic regression modeling

### Elastic net regression

For the RIDGE regression, I will start by finding the optimal lambda value using the function cv.glmnet(). I will do a 10-fold cross validation process to achieve this. 

```{r}
#convert training data to matrix format
x_matrix <- model.matrix(pcos_y_n~.,train)

#convert class to numerical variable
response <- ifelse(train$pcos_y_n=="1",1,0)

#perform grid search to find optimal value of lambda
ridge.cv <- cv.glmnet(x_matrix,response,alpha=0,
                      nfolds = 10,
                      family="binomial",type.measure = "class" )
#plot result
plot(ridge.cv)
```

After doing this search, I will use the lambda that gives the simplest model but also lies within one standard error of the optimal value of lambda for the rest of my analysis. 

```{r}
#Check the lambda that I will use
ridge.cv$lambda.1se

coef(ridge.cv,s=ridge.cv$lambda.1se)
```


### Random Forest 

## Model comparison 

*Select the best one at the end 

## Effect of class imbalance on the best model (if time allows)
