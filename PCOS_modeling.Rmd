---
title: "PCOS modeling"
author: "Erick Navarro & Timo Tolppa"
date: "2023-01-25"
output: 
  pdf_document:
    toc: true
    toc_depth: 4
---
# Creating the classificators

This report contains the details of the generation and comparison of several classifiers whose aim is to classify people with and without polycystic ovary syndrome (PCOS). This report is part of a larger project with a goal to develop and validate a model that takes easily collected variables, and tests its performance against models using variables obtained with increasingly invasive procedures. For a summary of the results and a clearer picture of the project, please read the Final_report.pdf file in this repository. 

## Setup
### Load packages and data 
```{r setup, include=FALSE}
# Load required libraries
library(tidyverse)
library(glmnet)
library(caret)
library(here)
library(mice)
library(DataExplorer)
library(cowplot)
library(vip)
library(pROC)

# Make the code reproducible
set.seed(504)

# Load the data
load(here("data.Rdata"))

# Overview of the data
glimpse(data)
```

### Data splitting 
After loading the data and packages, we will proceed to split the data into training and validation sets using a 70:30 random split.

```{r}
data = data %>% 
  column_to_rownames("id")

# Make the code reproducible
set.seed(504)

# Create the data split
train.index <- caret::createDataPartition(data$pcos, p = .7, list=FALSE)

# Define the training and validation data sets
train <- data[ train.index,]
valid <- data[-train.index,]

# Check the ratio of cases with and without the main outcome measure (i.e. PCOS diagnosis) in the training set
table(train$pcos)
table(train$pcos)[1]/ table(train$pcos)[2]

# Check the ratio of cases with and without the main outcome measure (i.e. PCOS diagnosis) in the validation set
table(valid$pcos)
table(valid$pcos)[1]/ table(valid$pcos)[2]

# Check the ratio of cases with and without the main outcome measure (i.e. PCOS diagnosis) in the original data set
table(data$pcos)
table(data$pcos)[1]/ table(data$pcos)[2]
```

The ratio of cases with and without the main outcome measure (i.e. PCOS diagnosis) is similar between the original (2.0565), validation (2.0566) and training (2.0565) data.

### Imputation on training set

Following the data splitting, we will impute missing values for the training set. As observed in the exploratory data analysis, the proportion of missing data is very low overall, which makes variables with missingness suitable for imputation.

```{r}
# Explore misingness in training set 
sapply(train, function(x) sum(is.na(x)))
```

For imputation, we will use the [mice R package](https://cran.r-project.org/web/packages/mice/mice.pdf), a widely used software to handle missing data. We will use the default options, which use the most appropiate methodology depending on the class of the data to be imputed, which are the following: "By default, the method uses pmm, predictive mean matching (numeric data) logreg, logistic regression imputation (binary data, factor with 2 levels) polyreg, polytomous regression imputation for un-ordered categorical data (factor > 2 levels) polr, proportional odds model for (ordered, > 2 levels)."

```{r, message=FALSE}
chained_train = mice::mice(train)

# Explore the imputed data and check that the generated value are plausible 
stripplot(chained_train, pulse_rate, pch = 19, xlab = "Imputation number")
stripplot(chained_train, lh, pch = 19, xlab = "Imputation number")
stripplot(chained_train, fsh_lh_ratio, pch = 19, xlab = "Imputation number")
stripplot(chained_train, amh, pch = 19, xlab = "Imputation number")
stripplot(chained_train, vitd3, pch = 19, xlab = "Imputation number")
stripplot(chained_train, fast_food, pch = 19, xlab = "Imputation number")
```

The values appear to be plausible, therefore, we will proceed with extracting the imputed data

```{r}
chained_train = complete(data=chained_train)
sapply(chained_train, function(x) sum(is.na(x)))
```


### Define the models

For the aim of this project, we will develop 4 different models with an increasing number of variables according to how easy they are to collect. Model 1 will contain only variables that are collected through patient history, model 2 will add variables collected through clinical examination, model 3 adds results of blood tests and model 4 will include all the relevant variables available in the data, including results from the transvaginal ultrasound. 

It is worth mentioning that we removed some non-relevant variables from the dataset based on the scientific literature of the field. A more detailed information about this selection can be found in the final report available in this repository. 

```{r}
# Create dataset 1 using only variables obtained through through patient history
model1_vars = c("pcos","age","cycle","cycle_length",
                 "no_of_abortions", "weight_gain", "hair_growth", "skin_darkening",
                 "hair_loss","pimples","fast_food", "reg_exercise")

# Create dataset 2 using variables obtained through patient history and clinical examination
model2_vars  = c(model1_vars, "weight","height","bmi",
                 "hip", "waist","waist_hip_ratio",
                 "bp_systolic", "bp_diastolic")

# Create dataset 3 using variables obtained through patient history, clinical examination and blood tests
model3_vars = c(model2_vars, "fsh", "lh", "fsh_lh_ratio",
              "amh", "prl", "vitd3", "prg", "rbs")

# Create dataset 4 using variables obtained through patient history, clinical examination, blood tests and ultrasound
model4_vars = c(model3_vars, "follicle_no_l", "follicle_no_r" ,
              "avg_f_size_l", "avg_f_size_r", "endometrium")

```

### Define cross-validation parameters

```{r}
# Set the cross-validation parameters for all models
fitControl <- trainControl(
    method = 'cv',
    number = 5,
    savePredictions = 'final',
    classProbs = TRUE,
    summaryFunction=twoClassSummary)
```

## Modeling

### Logistic regression modeling

First, we will fit a logistic regression model with the features mentioned above.

```{r}
# Set the seed to ensure reproducibility
set.seed(504)

#Since we have few samples, we will use bootstrapping instead of cross fold validation
models_logreg = list()

for (model in list(model1_vars, model2_vars, model3_vars, model4_vars)){
  cv_model = caret::train(
    pcos ~ ., 
    data = chained_train %>% 
      dplyr::select(all_of(model)), 
    method = "glm",
    family = "binomial",
    trControl = fitControl)
  models_logreg = append(models_logreg, list(cv_model))
}

names(models_logreg) = c("model1","model2", "model3", "model4")

models_logreg
```

### Elastic net regression

We will also fit an elastic net model, which is a method that performs feature selection and remove redundant terms in the models. 

```{r}

models_EN = list()

for (model in list(model1_vars, model2_vars, model3_vars, model4_vars)){
  en_model = caret::train(
    pcos ~ ., 
    trControl = fitControl,
    data = chained_train %>% 
      dplyr::select(all_of(model)), 
    method = "glmnet",
    tuneGrid = expand.grid(alpha = seq(0.1,.2,by = 0.05),  
                           lambda = seq(0.05,0.3,by = 0.05)),
    verbose = FALSE,
    metric="ROC")
  models_EN = append(models_EN, list(en_model))
}

names(models_EN) = c("model1","model2", "model3", "model4")
models_EN

#Plot parameter tuning of model 1
plot(models_EN[[1]])
#Plot parameter tuning of model 2
plot(models_EN[[2]])
#Plot parameter tuning of model 3
plot(models_EN[[3]])
#Plot parameter tuning of model 4
plot(models_EN[[4]])

```

### Random Forest 

Finally, we will fit a model of the CART family. 

```{r}
# Set the seed to ensure reproducibility
set.seed(504)

# Train the random forest model for all four sets of data
models_rf = list()

for (model in list(model1_vars, model2_vars, model3_vars, model4_vars)){
  rf_model = caret::train(
    pcos ~ ., 
    data = chained_train %>% 
      dplyr::select(all_of(model)),
    method = "ranger",
    num.trees = 500,
    tuneLength = 5, 
    metric = "Sens",
    trControl = fitControl,
    importance="impurity")
  models_rf = append(models_rf, list(rf_model))
}

names(models_rf) = c("model1","model2", "model3", "model4")
models_rf
```

The number of trees was kept at the default of 500, as increases in the number of trees to 1000, 5000 and 10000 improved model performance by less than 1% for ROC, sensitivity and specificity. However, the processing time was increased significantly and thus the default value was not changed. Main tuning parameters were determined using the inbuilt 'tuneLength' parameter by trying different default grid values for the main parameters, which were optimized to 'sensitivity' as per the aims of this project. Further detail and justification for the metrics used in this project can be found in the Final Report in this repository.

#### Random Forest: Variable Importance by Data Set

```{r}

# Create variable importance graphs for each data set
vip_rf_model1 <- vip::vip(models_rf[[1]])
vip_rf_model2 <- vip::vip(models_rf[[2]])
vip_rf_model3 <- vip::vip(models_rf[[3]])
vip_rf_model4 <- vip::vip(models_rf[[4]])

#Create a combined graph for all variable importance graphs
plot_grid(vip_rf_model1, vip_rf_model2, vip_rf_model3, vip_rf_model4, labels = c('Model 1', 'Model 2', 'Model 3', 'Model 4'), label_size = 10, vjust = 1, scale = 0.95)
```

## Model comparison 
## Test the performance in the validation set

For this step, we will first fit the models in the validation data set to get the performance metrics. 

We will concatenate all the models and estimate their performance in the validation set. 
```{r}
#Get the outcomes in the validation set.
outcomes = list(valid %>% 
                  dplyr::select(all_of(model1_vars)) %>% 
                  drop_na() %>% 
                  pull(pcos), 
                valid %>% 
                  dplyr::select(all_of(model2_vars)) %>% 
                  drop_na() %>% 
                  pull(pcos),
                valid %>% 
                  dplyr::select(all_of(model3_vars)) %>% 
                  drop_na() %>% 
                  pull(pcos),
                valid %>% 
                  dplyr::select(all_of(model4_vars)) %>% 
                  drop_na() %>% 
                  pull(pcos))

models_aggregated = c(models_logreg, models_EN, models_rf)

#Generate prediction metrics
pred_models = models_aggregated %>% 
  purrr::map(\(x) predict(object = x, newdata = valid, type = "raw")) %>% 
  purrr::map2(c(rep(outcomes,3)), \(x,y) caret::confusionMatrix(x,y)) 

#Create data frame with the metrics
(metrics = tibble(Method = c(rep("LR",4),
                            rep("EN",4),
                            rep("RF",4)),
                 Model = c(rep(c("1", "2","3", "4"), 3)),
                 Specificity = map(pred_models, "byClass") %>% 
                   map_dbl("Specificity"),
                 Sensitivity = map(pred_models, "byClass") %>% 
                   map_dbl("Sensitivity"),
                 F1 = map(pred_models, "byClass") %>% 
                   map_dbl("F1")
                 ))

```



```{r}



```



*Select the best one at the end 

## Effect of class imbalance on the best model (if time allows)
