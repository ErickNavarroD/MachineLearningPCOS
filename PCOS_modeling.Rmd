---
title: "PCOS modeling"
author: "Erick Navarro & Timo Tolppa"
date: "2023-01-25"
output: 
  pdf_document:
    toc: true
    toc_depth: 4
---
# Creating the classificators

This report contains the details of the generation and comparison of several classifiers whose aim is to classify people with and without polycystic ovary syndrome (PCOS). This report is part of a larger project with a goal to develop and validate a model that takes easily collected variables, and tests its performance against models using variables obtained with increasingly invasive procedures. For a summary of the results and a clearer picture of the project, please read the Final_report.pdf file in this repository. 

## Setup
### Load packages and data 
```{r setup, include=FALSE}
# Load required libraries
library(tidyverse)
library(glmnet)
library(caret)
library(here)
library(mice)
library(DataExplorer)
library(cowplot)
library(vip)
library(pROC)
library(pheatmap)
library(flextable)

# Make the code reproducible
set.seed(504)

# Load the data
load(here("data.Rdata"))

# Overview of the data
glimpse(data)
```

### Data splitting 
After loading the data and packages, we will proceed to split the data into training and validation sets using a 70:30 random split.

```{r}
data = data %>% 
  column_to_rownames("id")

# Make the code reproducible
set.seed(504)

# Create the data split
train.index <- caret::createDataPartition(data$pcos, p = .7, list=FALSE)

# Define the training and validation data sets
train <- data[ train.index,]
valid <- data[-train.index,]

# Check the ratio of cases with and without the main outcome measure (i.e. PCOS diagnosis) in the training set
table(train$pcos)
table(train$pcos)[1]/ table(train$pcos)[2]

# Check the ratio of cases with and without the main outcome measure (i.e. PCOS diagnosis) in the validation set
table(valid$pcos)
table(valid$pcos)[1]/ table(valid$pcos)[2]

# Check the ratio of cases with and without the main outcome measure (i.e. PCOS diagnosis) in the original data set
table(data$pcos)
table(data$pcos)[1]/ table(data$pcos)[2]
```

The ratio of cases with and without the main outcome measure (i.e. PCOS diagnosis) is similar between the original (2.0565), validation (2.0566) and training (2.0565) data.

### Imputation on training set

Following the data splitting, we will impute missing values for the training set. As observed in the exploratory data analysis, the proportion of missing data is very low overall, which makes variables with missingness suitable for imputation.

```{r}
# Explore misingness in training set 
sapply(train, function(x) sum(is.na(x)))
```

For imputation, we will use the [mice R package](https://cran.r-project.org/web/packages/mice/mice.pdf), a widely used software to handle missing data. We will use the default options, which use the most appropiate methodology depending on the class of the data to be imputed, which are the following: "By default, the method uses pmm, predictive mean matching (numeric data) logreg, logistic regression imputation (binary data, factor with 2 levels) polyreg, polytomous regression imputation for un-ordered categorical data (factor > 2 levels) polr, proportional odds model for (ordered, > 2 levels)."

```{r, message=FALSE, warning=FALSE}
chained_train = mice::mice(train)

# Explore the imputed data and check that the generated value are plausible 
stripplot(chained_train, pulse_rate, pch = 19, xlab = "Imputation number")
stripplot(chained_train, lh, pch = 19, xlab = "Imputation number")
stripplot(chained_train, fsh_lh_ratio, pch = 19, xlab = "Imputation number")
stripplot(chained_train, amh, pch = 19, xlab = "Imputation number")
stripplot(chained_train, vitd3, pch = 19, xlab = "Imputation number")
stripplot(chained_train, fast_food, pch = 19, xlab = "Imputation number")
```

The values appear to be plausible, therefore, we will proceed with extracting the imputed data

```{r}
chained_train = complete(data=chained_train)
sapply(chained_train, function(x) sum(is.na(x)))
```


### Define the models

For the aim of this project, we will develop 4 different models with an increasing number of variables according to how easy they are to collect. Model 1 will contain only variables that are collected through patient history, model 2 will add variables collected through clinical examination, model 3 adds results of blood tests and model 4 will include all the relevant variables available in the data, including results from the transvaginal ultrasound. 

It is worth mentioning that we removed some non-relevant variables from the dataset based on the scientific literature of the field. A more detailed information about this selection can be found in the final report available in this repository. 

```{r}
# Create dataset 1 using only variables obtained through through patient history
model1_vars = c("pcos","age","cycle","cycle_length",
                 "no_of_abortions", "weight_gain", "hair_growth", "skin_darkening",
                 "hair_loss","pimples","fast_food", "reg_exercise")

# Create dataset 2 using variables obtained through patient history and clinical examination
model2_vars  = c(model1_vars, "weight","height","bmi",
                 "hip", "waist","waist_hip_ratio",
                 "bp_systolic", "bp_diastolic")

# Create dataset 3 using variables obtained through patient history, clinical examination and blood tests
model3_vars = c(model2_vars, "fsh", "lh", "fsh_lh_ratio",
              "amh", "prl", "vitd3", "prg", "rbs")

# Create dataset 4 using variables obtained through patient history, clinical examination, blood tests and ultrasound
model4_vars = c(model3_vars, "follicle_no_l", "follicle_no_r" ,
              "avg_f_size_l", "avg_f_size_r", "endometrium")

```

### Define cross-validation parameters

```{r}
# Set the cross-validation parameters for all models
fitControl <- trainControl(
    method = 'cv',
    number = 5,
    savePredictions = 'final',
    classProbs = TRUE,
    summaryFunction=twoClassSummary)
```

## Modeling

### Logistic regression modeling

First, we will fit a logistic regression model with the features mentioned above.

```{r, warning=FALSE}
# Set the seed to ensure reproducibility
set.seed(504)

#Since we have few samples, we will use bootstrapping instead of cross fold validation
models_logreg = list()

for (model in list(model1_vars, model2_vars, model3_vars, model4_vars)){
  cv_model = caret::train(
    pcos ~ ., 
    data = chained_train %>% 
      dplyr::select(all_of(model)), 
    method = "glm",
    family = "binomial",
    trControl = fitControl)
  models_logreg = append(models_logreg, list(cv_model))
}

names(models_logreg) = c("model1","model2", "model3", "model4")

models_logreg
```

### Elastic net regression

We will also fit an elastic net model, which is a method that performs feature selection and remove redundant terms in the models. 

```{r}

models_EN = list()

for (model in list(model1_vars, model2_vars, model3_vars, model4_vars)){
  en_model = caret::train(
    pcos ~ ., 
    trControl = fitControl,
    data = chained_train %>% 
      dplyr::select(all_of(model)), 
    method = "glmnet",
    tuneGrid = expand.grid(alpha = seq(0.1,.2,by = 0.05),  
                           lambda = seq(0.05,0.3,by = 0.05)),
    verbose = FALSE,
    metric="ROC")
  models_EN = append(models_EN, list(en_model))
}

names(models_EN) = c("model1","model2", "model3", "model4")
models_EN

#Plot parameter tuning of model 1
plot(models_EN[[1]])
#Plot parameter tuning of model 2
plot(models_EN[[2]])
#Plot parameter tuning of model 3
plot(models_EN[[3]])
#Plot parameter tuning of model 4
plot(models_EN[[4]])

```

### Random Forest 

Finally, we will fit a model of the CART family. 

```{r}
# Set the seed to ensure reproducibility
set.seed(504)

# Train the random forest model for all four sets of data
models_rf = list()

for (model in list(model1_vars, model2_vars, model3_vars, model4_vars)){
  rf_model = caret::train(
    pcos ~ ., 
    data = chained_train %>% 
      dplyr::select(all_of(model)),
    method = "ranger",
    num.trees = 500,
    tuneLength = 5, 
    metric = "Sens",
    trControl = fitControl,
    importance="impurity")
  models_rf = append(models_rf, list(rf_model))
}

names(models_rf) = c("model1","model2", "model3", "model4")
models_rf
```

The number of trees was kept at the default of 500, as increases in the number of trees to 1000, 5000 and 10000 improved model performance by less than 1% for ROC, sensitivity and specificity. However, the processing time was increased significantly and thus the default value was not changed. Main tuning parameters were determined using the inbuilt 'tuneLength' parameter by trying different default grid values for the main parameters, which were optimized to 'sensitivity' as per the aims of this project. Further detail and justification for the metrics used in this project can be found in the Final Report in this repository.

## Model comparison 
## Test the performance in the validation set

For this step, we will first fit the models in the validation data set to get the performance metrics. 

We will concatenate all the models and estimate their performance in the validation set. 
```{r, message=FALSE, warning=FALSE}
#Get the outcomes in the validation set.
outcomes = list(valid %>% 
                  dplyr::select(all_of(model1_vars)) %>% 
                  drop_na() %>% 
                  pull(pcos), 
                valid %>% 
                  dplyr::select(all_of(model2_vars)) %>% 
                  drop_na() %>% 
                  pull(pcos),
                valid %>% 
                  dplyr::select(all_of(model3_vars)) %>% 
                  drop_na() %>% 
                  pull(pcos),
                valid %>% 
                  dplyr::select(all_of(model4_vars)) %>% 
                  drop_na() %>% 
                  pull(pcos))

models_aggregated = c(models_logreg, models_EN, models_rf)

#Generate prediction metrics
pred_models = models_aggregated %>% 
  purrr::map(\(x) predict(object = x, newdata = valid, type = "raw")) %>% 
  purrr::map2(c(rep(outcomes,3)), \(x,y) caret::confusionMatrix(x,y)) 

AUC = models_aggregated %>% 
  purrr::map(\(x) predict(object = x, newdata = valid, type = "prob")) %>% 
  purrr::map2(c(rep(outcomes,3)), \(x,y) pROC::auc(y, x$Yes)) 

#Create data frame with the metrics
(metrics = tibble(Method = c(rep("LR",4),
                            rep("EN",4),
                            rep("RF",4)),
                 Model = c(rep(c("1", "2","3", "4"), 3)),
                 Specificity = map(pred_models, "byClass") %>% 
                   map_dbl("Specificity"),
                 Sensitivity = map(pred_models, "byClass") %>% 
                   map_dbl("Sensitivity"),
                 F1 = map(pred_models, "byClass") %>% 
                   map_dbl("F1"),
                 AUC = unlist(AUC)
                 ))

pheatmap(metrics %>% 
           unite(name, Method, Model, sep = "") %>% 
           column_to_rownames(var= "name") %>% 
           select(c(Sensitivity, Specificity, F1, AUC)),
         cluster_rows = F,
         cluster_cols = F, 
         scale = "column",
         display_numbers = round(metrics %>% 
           select(c(Sensitivity, Specificity, F1, AUC)),2)
         )
```


In the plot above, we can observe that models 1,2 and 3 have a very similar performance in all of the methods applied. Model 4 has consistently a better performance than 1/2/3, which suggests that ultrasound variables are informative. However, getting that information usually requires a more specialized medical equipement, which is not that easily accessible. 

If we look at Sensitivity and F1, we can see that model 1 usually outperforms models 2 and 3. Since the aim of our project is to create a model that balances performance and easily collected variables, model 1 is the best option. 

Furthermore, out of all of the models 1, the method that creates the best model is Elastic Net based on the metrics. Therefore, we will propose it as the final model in our project. 

## Explore EN model 1
### Variable importance
Next, we will explore the most important variables in the elastic net models 

```{r}
#Create a combined graph for all variable importance graphs
plot_grid(vip::vip(models_EN[[1]]), 
          vip::vip(models_EN[[2]]), 
          vip::vip(models_EN[[3]]), 
          vip::vip(models_EN[[4]]), labels = c('Model 1', 'Model 2', 'Model 3', 'Model 4'), label_size = 10, vjust = 1, scale = 0.95)
```

### Effect of imputation

As a post hoc analysis, we wanted to explore how the model would've performed if we had used only complete cases and hadn't had performed any imputation. 

```{r}
set.seed(504)
train_complete = train %>% 
  drop_na()

models_EN = list()

en_model1_complete = caret::train(
    pcos ~ ., 
    trControl = fitControl,
    data = train_complete %>% 
      dplyr::select(all_of(model1_vars)), 
    method = "glmnet",
    tuneGrid = expand.grid(alpha = seq(0.1,.2,by = 0.05),  
                           lambda = seq(0.05,0.3,by = 0.05)),
    verbose = FALSE,
    metric="ROC")

pred_en1_complete = predict(object = en_model1_complete, newdata = valid, type = "raw") %>% 
  confusionMatrix(outcomes[[1]])

AUC_en1_models = predict(object = en_model1_complete, newdata = valid, type = "prob")
AUC_en1_models = pROC::auc(outcomes[[1]], AUC_en1_models$Yes) 

(comparison_complete = metrics %>% 
  unite(Name, Method, Model, sep = "") %>% 
  filter(Name == "EN1") %>% 
  bind_rows(tibble(Name = "EN1_complete",
                       Specificity = pred_en1_complete$byClass["Specificity"],
                       Sensitivity = pred_en1_complete$byClass["Sensitivity"],
                       F1 = pred_en1_complete$byClass["F1"],
                       AUC = as.numeric(AUC_en1_models))))

comparison_complete %>% 
  flextable::flextable() %>% 
  flextable::colformat_double(digits = 3)
  
```


### Class imbalance

Finally, we will exlpore the effect of correcting for class imbalance in our dataset in the elastic net model 1. 
```{r}
fitControl$sampling <- "down"

down_fit <- train(pcos ~ .,
                  data = chained_train %>% 
                    select(all_of(model1_vars)),
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = seq(0.1,.2,by = 0.05),  
                                         lambda = seq(0.05,0.3,by = 0.05)),
                  verbose = FALSE,
                  metric="ROC",
                  trControl = fitControl)

pred_en1_down = predict(object = down_fit, newdata = valid, type = "raw") %>% 
  confusionMatrix(outcomes[[1]])

AUC_down = predict(object = down_fit, newdata = valid, type = "prob")
AUC_down = pROC::auc(outcomes[[1]], AUC_down$Yes) 

(comparison_down = metrics %>% 
  unite(Name, Method, Model, sep = "") %>% 
  filter(Name == "EN1") %>% 
  bind_rows(tibble(Name = "EN1_down",
                       Specificity = pred_en1_down$byClass["Specificity"],
                       Sensitivity = pred_en1_down$byClass["Sensitivity"],
                       F1 = pred_en1_down$byClass["F1"],
                       AUC = as.numeric(AUC_down))))

comparison_down %>% 
  flextable::flextable() %>% 
  flextable::colformat_double(digits = 3)

```

As we can observe, the model without the class imbalance correction has a higher sensitivity, and F1 score, which are metrics that we chose to maximize because of the aim of our study. Therefore, we will keep the EN model 1 as the proposed model of our study

## R sesion

```{r}
sessionInfo()
```

