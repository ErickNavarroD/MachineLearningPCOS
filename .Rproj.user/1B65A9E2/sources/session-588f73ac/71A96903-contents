---
title: "Pima data"
author: "Aline Talhouk"
date: "2023-01-13"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## The PIMA dataset
For this example we will use the Pima dataset, included in the MASS library:

>A population of women who were at least 21 years old, of Pima Indian heritage and living near Phoenix, Arizona, was tested for diabetes according to World Health Organization criteria. The data were collected by the US National Institute of Diabetes and Digestive and Kidney Diseases. We used the 532 complete records after dropping the (mainly missing) data on serum insulin.

The dataset includes the following variables:

* npreg: Number of times pregnant
* glu: Plasma glucose concentration a 2 hours in an oral glucose tolerance test
* bp: Diastolic blood pressure (mm Hg)
* skin: Triceps skin fold thickness (mm)
* bmi: Body mass index (weight in kg/(height in m)^2)
* ped: Diabetes pedigree function
* age: Age (years)
* type: Yes or No (diabetes)

The outcome variable is type, indicating whether a person has diabetes. 

The data is split into a train and test set, which we will combine together.
```{r}
library(tidyverse)
library(gridExtra)
library(plotROC)
library(arm)
library(MASS)
data("Pima.tr")
data("Pima.te")

d <- rbind(Pima.te, Pima.tr)

glimpse(d)
```
All the predictors are either integer or numeric. Our objective is to build a logistic model of diabetes to practice interpreting coefficients.

```{r}
ggplot(d, aes(x = bmi, y = age, col = type)) +
  geom_point(alpha = .4) +
  ggtitle("BMI vs. Age by diabetes status")
```

```{r}
grid.arrange(
ggplot(d, aes(type, bmi)) + 
  geom_boxplot() +
  ggtitle("BMI by Diabetes status") + 
  ylab("BMI")+
  xlab("Diabetes"),


ggplot(d, aes(type, age)) + 
  geom_boxplot() +
  ggtitle("Age by Diabetes status") + 
  ylab("Age")+
  xlab("Diabetes"),
ncol = 2)
```
High values of age and BMI are associated with higher incidence of diabetes 

#Simple model
```{r}
bin_model1 <- 
  glm(type ~ bmi + age, 
      data = d, 
      family = binomial)

display(bin_model1)
```
How to interpret the output?

# Assessing model fit
We can assess logistic model performance using AIC as well as with several other measures of model fit, such as residual deviance, accuracy, sensitivity, specificity and area under the curve (AUC).

Like AIC, deviance is a measure of error, so lower deviance means a better fit to the data. We expect deviance to decline by 1 for every predictor, so with an informative predictor deviance will decline by more than 1. Deviance =  âˆ’2ln(L), where  ln
  is the natural log and  L
  is the likelihood function.
  
```{r}
logistic_model1 <- 
  glm(type ~ bmi, 
      data = d, 
      family = binomial)
logistic_model1$deviance
```


```{r}
logistic_model2 <- 
glm(type ~ bmi+age, 
      data = d, 
      family = binomial)
logistic_model2$deviance
```
In this case, deviance declined, indicating that age improves model fit. We can do a formal test of the difference using, as for linear models, the likelihood ratio test:

```{r}
lmtest::lrtest(logistic_model1, logistic_model2)
```

```{r}
set.seed(1)
train.index <- caret::createDataPartition(Pima.tr$type, p = .7, list=FALSE)
train <- Pima.tr[ train.index,]
valid  <- Pima.tr[-train.index,]

summary(train$type)
```

```{r}
summary(valid$type)
```


```{r}
pima_mod_1 <- glm(type ~ bmi+ age, train, family=binomial(link="logit"))
summary(pima_mod_1)
```
```{r}
pima_mod_2 <- glm(type ~ .  , train, family=binomial(link="logit"))
summary(pima_mod_2)
```

#ROC and AUC
```{r}
plot(pROC::roc(train$type,
                   fitted(pima_mod_2)),
               col = "red", 
               main = "ROC curves: logistic model 1 (red) vs. logistic model 2 (blue)")
plot(pROC::roc(train$type,
                   fitted(pima_mod_1)),
               print.auc = T, 
               col = "blue", 
               add = T)
```

```{r}
fit <- glm(type ~ ., data = train, family = "binomial")

df <- data.frame(Status = train$type, Prob = fit$fitted.values)
ggplot(df, aes(d = Status, m = Prob)) + geom_roc()
```
The y-axis of the ROC plot depicts the true positive rate, or sensitivity, while the x-axis shows the false positive rate, which is the same as 1-specificity. The curve depicts the sensitivity and 1-specificity that result from different decision thresholds, which are marked at various points.

```{r}
binnedplot(fitted(pima_mod_2), 
           residuals(pima_mod_2, type = "response"), 
           nclass = NULL, 
           xlab = "Expected Values", 
           ylab = "Average residual", 
           main = "Binned residual plot", 
           cex.pts = 0.8, 
           col.pts = 1, 
           col.int = "gray")
```
```{r}
pima_mod_3 <- 
  standardize(glm(type ~ bmi  + ped * glu +  age + npreg + bp , 
      data = d, 
      family = binomial)) 
summary(pima_mod_3)
```


## Validation Error
```{r}
tr_prob_1 <- predict(pima_mod_1, train, type="response")
tr_pred_1 <- ifelse(tr_prob_1 < 0.5, "No", "Yes") %>% factor()
tr_acc_1 <- mean(tr_pred_1 == train$type)

va_prob_1 <- predict(pima_mod_1, valid, type="response")
va_pred_1 <- ifelse(va_prob_1 < 0.5, "No", "Yes") %>% factor()
va_acc_1 <- mean(va_pred_1 == valid$type)
```

```{r}
tr_prob_2 <- predict(pima_mod_2, train, type="response")
tr_pred_2 <- ifelse(tr_prob_2 < 0.5, "No", "Yes") %>% factor()
tr_acc_2 <- mean(tr_pred_2 == train$type)

va_prob_2 <- predict(pima_mod_2, valid, type="response")
va_pred_2 <- ifelse(va_prob_2 < 0.5, "No", "Yes") %>% factor()
va_acc_2 <- mean(va_pred_2 == valid$type)
```

```{r}
tr_prob_3 <- predict(pima_mod_3, train, type="response")
tr_pred_3 <- ifelse(tr_prob_3 < 0.5, "No", "Yes") %>% factor()
tr_acc_3 <- mean(tr_pred_3 == train$type)

va_prob_3 <- predict(pima_mod_3, valid, type="response")
va_pred_3 <- ifelse(va_prob_3 < 0.5, "No", "Yes") %>% factor()
va_acc_3 <- mean(va_pred_3 == valid$type)
```

```{r}
cat("Model 1 Training Accuracy:   ", tr_acc_1, "\n",
    "Model 1 Validation Accuracy: ", va_acc_1, "\n\n",
    "Model 2 Training Accuracy:   ", tr_acc_2, "\n",
    "Model 2 Validation Accuracy: ", va_acc_2, "\n\n",
    "Model 3 Training Accuracy:   ", tr_acc_3, "\n",
    "Model 3 Validation Accuracy: ", va_acc_3, "\n\n", sep=""
)
```
## Cross-validation
```{r}
cv <- caret::trainControl(
  method = "repeatedcv", 
  number = 5, #five fold cross validation 
  repeats = 3 # repeat 5 times
)
set.seed(123)
cv_model1 <- caret::train(
  type ~ bmi, 
  data = train, 
  method = "glm",
  family = "binomial",
  trControl = cv
)
set.seed(123)
cv_model2 <- train(
  type ~ bmi + age, 
  data = train, 
  method = "glm",
  family = "binomial",
  trControl = cv
)

set.seed(123)
cv_model3 <- train(
  type ~ ., 
  data = train, 
  method = "glm",
  family = "binomial",
  trControl = cv
)

summary(
  resamples(
    list(
      model1 = cv_model1, 
      model2 = cv_model2, 
      model3 = cv_model3
    )
  )
)$statistics$Accuracy
```
```{r}
set.seed(849)
ridge_caret<- train(train[,-8],train[,8], method = "glmnet",
                trControl=cv,
                tuneGrid = expand.grid(alpha = 0,
                                       lambda = seq(0.001,0.1,by = 0.001)))
ridge_caret$bestTune
coef(ridge_caret$finalModel, ridge_caret$bestTune$lambda)
plot(ridge_caret)
```

```{r}
lasso_caret<- train(train[,-8],train[,8], method = "glmnet",
                trControl=cv,
                tuneGrid = expand.grid(alpha = 1,
                                       lambda = seq(0.001,0.1,by = 0.001)))

lasso_caret$bestTune
coef(lasso_caret$finalModel, lasso_caret$bestTune$lambda)
plot(lasso_caret)
```

## Present the data with finalfit
```{r}
library(finalfit)
dependent <-"type" 
explanatory <-c("bmi","age")

d %>%
finalfit(dependent, explanatory, metrics = TRUE)
```

